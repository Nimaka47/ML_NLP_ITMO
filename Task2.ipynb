{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d85fdc0-0a1f-49cd-96a0-fc55b04b3042",
   "metadata": {},
   "source": [
    "# –ó–∞–¥–∞–Ω–∏–µ 2\n",
    "\n",
    "**–î–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–≤–æ–µ–π –∑–∞–¥–∞—á–∏.** –ù–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å, –≤–Ω–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–¥–∞—á–∏ –≤ NLP, –≤–µ—Ä–æ—è—Ç–Ω–µ–µ –≤—Å–µ–≥–æ, –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –±—É–¥–µ—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –ø–æ—ç—Ç–æ–º—É –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:\n",
    "\n",
    "- <font color='green'>(status)</font> –≤—ã–±—Ä–∞—Ç—å –≤ HuggingFace Hub –º–æ–¥–µ–ª—å, –ø–æ–¥—Ö–æ–¥—è—â—É—é –¥–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏\n",
    "- <font color='green'>(status)</font> –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- <font color='green'>(status)</font> –∑–∞–º–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ –¥–æ –∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99beb02-7c1d-4864-83ff-ad8fc9405782",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afb97962-a01a-4547-981d-2bf3e2eec671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import datasets\n",
    "from scipy.special import softmax\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c28b9-df2a-4be1-b9c5-50bb065e7652",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "602d36dd-3054-4d0a-a059-6a95ac14d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_preparation(csv_file_name):\n",
    "    df = pd.read_csv(csv_file_name)\n",
    "\n",
    "    df = df.set_axis(['id', 'entity', 'sentiment', 'text'], axis=1)\n",
    "    df = df[df['sentiment'] != 'Irrelevant']\n",
    "    df['label'] = df['sentiment'].map({'Positive': 2, 'Neutral': 1, 'Negative': 0, 'Irrelevant': -1})\n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    df = df.drop(columns=['id', 'entity', 'sentiment'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "053956f9-4b15-4cdd-8269-d5672dbd46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_preparation('twitter_validation.csv')\n",
    "df_train = df_preparation('twitter_training.csv')\n",
    "\n",
    "dataset = datasets.DatasetDict({\"train\":datasets.Dataset.from_dict(df_train),\"test\":datasets.Dataset.from_dict(df_valid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1448ca9e-ad3b-495f-acae-29ee8f7506d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\artem.larin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3465e4a11abf431e802b3e8901c0206f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61691 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93330783b03419d890b1da7facd7808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "# model_name = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "model_name = f\"elozano/tweet_sentiment_eval\"\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c838030",
   "metadata": {},
   "source": [
    "# Metrics before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff1aab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.5% - 75.0 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.62      0.71      0.66       266\n",
      "     Neutral       0.53      0.29      0.37       285\n",
      "    Positive       0.61      0.82      0.70       277\n",
      "\n",
      "    accuracy                           0.60       828\n",
      "   macro avg       0.59      0.60      0.58       828\n",
      "weighted avg       0.59      0.60      0.57       828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_running = dataset['test']\n",
    "reference = []\n",
    "prediction = []\n",
    "count_full = dataset_running.shape[0]\n",
    "start_time = time.time()\n",
    "for count, ds in enumerate(dataset_running):\n",
    "    encoded_input = tokenizer(ds['text'], return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = softmax(output[0][0].detach().numpy())\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "\n",
    "    prediction.append(ranking[0])\n",
    "    reference.append(ds['label'])\n",
    "    if count % (round(count_full*0.01)) == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{np.round(count / count_full * 100, 1)}% - {np.round(time.time() - start_time)} sec\")\n",
    "\n",
    "print(classification_report(reference, prediction, target_names=['Negative', 'Neutral', 'Positive']))\n",
    "\n",
    "del reference, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496c9c5",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7bf5c27d-ada3-4df0-ab02-8fde4e169756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\artem.larin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(int(tokenized_datasets[\"train\"].shape[0] * 0.02)))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(int(tokenized_datasets[\"test\"].shape[0] * 1.0)))\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "save_directory = './pt_save_pretrained'\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='test_trainer', \n",
    "    evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa87ab1b-8795-48fd-ac8a-b72a707e4809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89da3f1ed22a4cf1bf5e381fcd51c4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62701475bba4984ae2e2939433cc0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7827932834625244, 'eval_accuracy': 0.6859903381642513, 'eval_runtime': 1666.101, 'eval_samples_per_second': 0.497, 'eval_steps_per_second': 0.062, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03afb94861d648beb56fa5f05c224ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8389763832092285, 'eval_accuracy': 0.6956521739130435, 'eval_runtime': 1666.8506, 'eval_samples_per_second': 0.497, 'eval_steps_per_second': 0.062, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779cef85b4af4e39b8f945ce1b486af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.919695258140564, 'eval_accuracy': 0.7125603864734299, 'eval_runtime': 1684.217, 'eval_samples_per_second': 0.492, 'eval_steps_per_second': 0.062, 'epoch': 3.0}\n",
      "{'train_runtime': 20135.1046, 'train_samples_per_second': 0.184, 'train_steps_per_second': 0.023, 'train_loss': 0.6590914818548387, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=465, training_loss=0.6590914818548387, metrics={'train_runtime': 20135.1046, 'train_samples_per_second': 0.184, 'train_steps_per_second': 0.023, 'total_flos': 973256532175872.0, 'train_loss': 0.6590914818548387, 'epoch': 3.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "590a74e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "config.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299f037",
   "metadata": {},
   "source": [
    "# Metrics after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e040087",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a73abae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.5% - 75.0 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.70      0.79      0.74       266\n",
      "     Neutral       0.71      0.57      0.63       285\n",
      "    Positive       0.72      0.78      0.75       277\n",
      "\n",
      "    accuracy                           0.71       828\n",
      "   macro avg       0.71      0.72      0.71       828\n",
      "weighted avg       0.71      0.71      0.71       828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_running = dataset['test']\n",
    "reference = []\n",
    "prediction = []\n",
    "count_full = dataset_running.shape[0]\n",
    "start_time = time.time()\n",
    "for count, ds in enumerate(dataset_running):\n",
    "    encoded_input = tokenizer(ds['text'], return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = softmax(output[0][0].detach().numpy())\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "\n",
    "    prediction.append(ranking[0])\n",
    "    reference.append(ds['label'])\n",
    "    if count % (round(count_full*0.01)) == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{np.round(count / count_full * 100, 1)}% - {np.round(time.time() - start_time)} sec\")\n",
    "\n",
    "print(classification_report(reference, prediction, target_names=['Negative', 'Neutral', 'Positive']))\n",
    "\n",
    "del reference, prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
